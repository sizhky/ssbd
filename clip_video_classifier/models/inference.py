# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/model/03_infer.ipynb.

# %% auto 0
__all__ = ["Inference"]

# %% ../../nbs/model/03_infer.ipynb 3
from ..preprocess.video_to_frames import video_to_frames
from clip_video_classifier.models.transformer_encoder import (
    TransformerEncoder,
    collate_fn,
)
from .frame_embeddings import Frame2Embeddings
from ..data.dataset import ClipEmbeddingsDataset
from transformers import Trainer, TrainingArguments
from torch_snippets import *
from functools import lru_cache

# %% ../../nbs/model/03_infer.ipynb 4
from torch_snippets.trainer import to


class Inference:
    def __init__(self, model_path):
        self.model, self.clip = self.load_model_and_clip(model_path)

    def load_model_and_clip(self, model_path):
        model = TransformerEncoder(4, 512, 128)
        load_torch_model_weights_to(model, model_path, device="cpu")
        model.eval()
        clip = Frame2Embeddings(device="cpu")
        return model, clip

    @torch.no_grad()
    def predict_on_video_path(self, video_path):
        frames = video_to_frames(
            video_path, frames_path=None, start_sec=0, clip_duration=5, verbose=True
        )
        frames = to(frames, self.clip.device)
        input = self.clip.frames2clip_image_embeddings(frames)
        input = {"embeddings": input}
        Info(f"{input=}")
        input = collate_fn([input])
        predictions = self.model(**input)
        predictions = F.softmax(predictions["logits"], dim=-1)
        Info(f"{predictions=}")
        confidence, predicted_class = predictions.max(1)
        predicted_class = ClipEmbeddingsDataset.id2label[predicted_class.item()]
        confidence = confidence.item()
        Info(f"{predicted_class=} @ {confidence=}")
        return {
            "predicted_class": predicted_class,
            "confidence": f"{confidence:.2f}",
        }
