# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/model/02_transformer_clip.ipynb.

# %% auto 0
__all__ = ["PositionalEncoding", "TransformerEncoder", "collate_fn"]

# %% ../../nbs/model/02_transformer_clip.ipynb 3
from ..cli import cli
from ..data.dataset import ClipEmbeddingsDataset
from torch_snippets import *
from torch.nn.utils.rnn import pad_sequence


# %% ../../nbs/model/02_transformer_clip.ipynb 5
class PositionalEncoding(nn.Module):
    def __init__(self, hidden_size: int, dropout=0.1, max_len=512):
        super(PositionalEncoding, self).__init__()

        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, hidden_size)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.pow(
            1e4, -torch.arange(0, hidden_size, 2).float() / hidden_size
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:, : x.size(1)]
        return self.dropout(x)


class TransformerEncoder(nn.Module):
    def __init__(
        self,
        transformer_layers: int,
        emb_size: int,
        max_len: int,
        num_classes: int = 4,
        d_model: int = 512,
        n_head: int = 8,
    ):
        super().__init__()
        self.positional_encoding = PositionalEncoding(
            hidden_size=emb_size, max_len=max_len
        )
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer, num_layers=transformer_layers
        )
        self.linear = nn.Linear(d_model, num_classes)
        self.main_input_name = "embeddings"
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, embeddings, attention_mask, targets=None):
        embeddings = self.positional_encoding(embeddings)
        transformer_output = self.transformer_encoder(
            embeddings.swapaxes(1, 0), src_key_padding_mask=attention_mask.bool()
        ).swapaxes(1, 0)
        pooled_output = transformer_output.mean(dim=1)
        logits = self.linear(pooled_output)
        if targets is not None:
            loss = self.loss_fn(logits, targets)
        else:
            loss = -1
        return {"loss": loss, "logits": logits}


def collate_fn(batch, seq_len=128):
    embeddings = [item["embeddings"] for item in batch]
    Debug(f"{embeddings=}")
    starts = [randint(len(i) - seq_len) if len(i) > seq_len else 0 for i in embeddings]
    Debug(f"{starts=}")
    embeddings = [
        e if len(e) < seq_len else e[starts[ix] : starts[ix] + seq_len]
        for ix, e in enumerate(embeddings)
    ]
    Debug(f"{embeddings=}")
    batched_embeddings = pad_sequence(embeddings, batch_first=True).float()
    Debug(f"{batched_embeddings=}")
    batch_size, seq_len, _ = batched_embeddings.shape
    attention_mask = torch.zeros((batch_size, seq_len), dtype=torch.float)
    Debug(f"{attention_mask=}")
    for i, seq in enumerate(embeddings):
        attention_mask[i, : len(embeddings)] = 1
    Debug(f"{attention_mask=}")
    if "targets" in batch[0]:
        Debug(f"has targets")
        labels = [item["targets"] for item in batch]
        Debug("{labels=}")
        return {
            "embeddings": batched_embeddings,
            "attention_mask": attention_mask,
            "targets": torch.Tensor(labels).long(),
        }
    else:
        Debug(f"not has targets")
        return {
            "embeddings": batched_embeddings,
            "attention_mask": attention_mask,
        }
