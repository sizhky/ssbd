[
  {
    "objectID": "model/backups/02_transformer/changelog.html#transformer__0000-1",
    "href": "model/backups/02_transformer/changelog.html#transformer__0000-1",
    "title": "clip-video-classifier",
    "section": "02_transformer__0000",
    "text": "02_transformer__0000"
  },
  {
    "objectID": "model/07.02_transformer_timm_resnet50.html",
    "href": "model/07.02_transformer_timm_resnet50.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from clip_video_classifier.cli import cli\nfrom functools import lru_cache\nfrom torch_snippets import *\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass FeaturesDataset(Dataset):\n    labels = [\"ArmFlapping\", \"HeadBanging\", \"Spinning\", \"others\"]\n    label2id = {l: ix for ix, l in enumerate(labels)}\n    id2label = {ix: l for l, ix in label2id.items()}\n\n    def __init__(\n        self,\n        features_dir: str,\n        annotations: str,\n        average_features: bool = False,\n        frames_dir: str = None,\n        binary_mode: bool = False,\n    ):\n        self.average_features = average_features\n        self.features_dir = P(features_dir)\n        if isinstance(annotations, (str, P)):\n            self.annotations = pd.read_csv(annotations)\n        else:\n            self.annotations = annotations\n        available_features = [\n            int(stem(f).split(\".\")[0]) for f in self.features_dir.ls()\n        ]\n        available_annotations = self.annotations.index.tolist()\n        self.annotations = self.annotations.loc[\n            list(common(available_annotations, available_features))\n        ]\n        self.frames_dir = frames_dir\n        self.binary_mode = binary_mode\n        Info(f\"created dataset of {len(self)} items\")\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def preprocess(self, features):\n        return features\n\n    # @lru_cache\n    def __getitem__(self, index):\n        row = self.annotations.iloc[index]\n        features = loaddill(\n            self.features_dir / f\"{row.name}.frames.features.tensor\"\n        ).cpu()\n        features = self.preprocess(features)\n        label = row[\"label\"]\n        if self.binary_mode:\n            label = int(label != \"others\")\n        else:\n            label = self.label2id[label]\n        if 0:\n            frames = loaddill(self.frames_dir / f\"{row.name}.frames.tensor\")\n        return {\n            \"features\": features.cpu().detach(),\n            \"targets\": label,\n        }\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\n\nfeatures_dim = 512\nfeatures_dir = root / \"ssbd-frames-features/10fps/resnet50\"\nmodel_output_dir = \"./transformers_model_trained_resnet50\"\n\nannotations_path = root / \"annotations.csv\"\nannotations = pd.read_csv(annotations_path)\nBINARY_MODE = False\n\nds = FeaturesDataset(features_dir, annotations)\nds[0]\n\n\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.15, random_state=11\n)\n\n# trn_df = annotations.loc[\n#     annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n# ]\ntrn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n\nds = FeaturesDataset(features_dir, annotations, binary_mode=BINARY_MODE)\ntrn_ds = FeaturesDataset(features_dir, trn_df, binary_mode=BINARY_MODE)\nval_ds = FeaturesDataset(features_dir, val_df, binary_mode=BINARY_MODE)\nprint(\n    \"train\",\n    collections.Counter([i[\"targets\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"targets\"] for i in track2(val_ds)]),\n)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size: int, dropout=0.1, max_len=512):\n        super(PositionalEncoding, self).__init__()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, hidden_size)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.pow(\n            1e4, -torch.arange(0, hidden_size, 2).float() / hidden_size\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass TransformerEncoderForTimmFeatures(nn.Module):\n    def __init__(\n        self,\n        transformer_layers: int,\n        emb_size: int,\n        max_len: int,\n        features_dim: int,\n        num_classes: int = 4,\n        d_model: int = 512,\n        n_head: int = 8,\n    ):\n        super().__init__()\n        self.positional_encoding = PositionalEncoding(\n            hidden_size=emb_size, max_len=max_len\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer=encoder_layer, num_layers=transformer_layers\n        )\n        self.lin = nn.Sequential(\n            nn.Linear(features_dim, emb_size),\n            nn.ReLU(inplace=True),\n        )\n        self.linear = nn.Linear(d_model, num_classes)\n        self.main_input_name = \"features\"\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.dropout = 0.25\n\n    def forward(self, features, attention_mask, targets=None):\n        Debug(f\"{features=}\")\n        features = self.lin(features)\n        embeddings = self.positional_encoding(features)\n        # embeddings = F.dropout(embeddings, self.dropout)\n        transformer_output = self.transformer_encoder(\n            embeddings.swapaxes(1, 0), src_key_padding_mask=attention_mask.bool()\n        ).swapaxes(1, 0)\n        # transformer_output = F.dropout(transformer_output, self.dropout)\n        pooled_output = transformer_output.mean(dim=1)\n        logits = self.linear(pooled_output)\n        if targets is not None:\n            loss = self.loss_fn(logits, targets)\n        else:\n            loss = -1\n        return {\"loss\": loss, \"logits\": logits}\n\n\ndef collate_fn(batch):\n    seq_len = 128\n    embeddings = [item[\"features\"] for item in batch]\n    Debug(f\"{embeddings=}\")\n    starts = [randint(len(i) - seq_len) if len(i) &gt; seq_len else 0 for i in embeddings]\n    Debug(f\"{starts=}\")\n    embeddings = [\n        e if len(e) &lt; seq_len else e[starts[ix] : starts[ix] + seq_len]\n        for ix, e in enumerate(embeddings)\n    ]\n    Debug(f\"{embeddings=}\")\n    batched_embeddings = pad_sequence(embeddings, batch_first=True).float()\n    Debug(f\"{batched_embeddings=}\")\n    batch_size, seq_len, *_ = batched_embeddings.shape\n    attention_mask = torch.zeros((batch_size, seq_len), dtype=torch.float)\n    Debug(f\"{attention_mask=}\")\n    for i, seq in enumerate(embeddings):\n        attention_mask[i, : len(embeddings)] = 1\n    Debug(f\"{attention_mask=}\")\n    if \"targets\" in batch[0]:\n        Debug(f\"has targets\")\n        labels = [item[\"targets\"] for item in batch]\n        Debug(f\"{labels=}\")\n        return {\n            \"features\": batched_embeddings[..., ::4],\n            \"attention_mask\": attention_mask,\n            \"targets\": torch.Tensor(labels).long(),\n        }\n    else:\n        Debug(f\"not has targets\")\n        return {\n            \"embeddings\": batched_embeddings[..., ::4],\n            \"attention_mask\": attention_mask,\n        }\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nmodel = TransformerEncoderForTimmFeatures(4, features_dim, 128, d_model=features_dim)\nwith debug_mode():\n    b = next(iter(dl))\n    o = model(**b)\nprint(o)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoderForTimmFeatures(\n    4, features_dim, 512, features_dim, d_model=features_dim\n)\nreset_logger()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    id2label = trn_ds.id2label if not BINARY_MODE else [\"others\", \"action\"]\n    pred = np.array([id2label[p] for p in pred])\n    targets = np.array([id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=model_output_dir,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    max_steps=2000,\n    logging_steps=200,\n    save_steps=200,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    eval_accumulation_steps=1,\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 30,  # Adjust this as needed\n    # \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n\n_ = trainer.predict(val_ds)\n\n\n_ = trainer.predict(trn_ds)\n\n\nbackup_this_notebook(\"07.02_transformer_timm_resnet50.ipynb\")"
  },
  {
    "objectID": "model/07.00_transformer_timm_resnet18.html",
    "href": "model/07.00_transformer_timm_resnet18.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from clip_video_classifier.cli import cli\nfrom functools import lru_cache\nfrom torch_snippets import *\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass FeaturesDataset(Dataset):\n    labels = [\"ArmFlapping\", \"HeadBanging\", \"Spinning\", \"others\"]\n    label2id = {l: ix for ix, l in enumerate(labels)}\n    id2label = {ix: l for l, ix in label2id.items()}\n\n    def __init__(\n        self,\n        features_dir: str,\n        annotations: str,\n        average_features: bool = False,\n        frames_dir: str = None,\n        binary_mode: bool = False,\n    ):\n        self.average_features = average_features\n        self.features_dir = P(features_dir)\n        if isinstance(annotations, (str, P)):\n            self.annotations = pd.read_csv(annotations)\n        else:\n            self.annotations = annotations\n        available_features = [\n            int(stem(f).split(\".\")[0]) for f in self.features_dir.ls()\n        ]\n        available_annotations = self.annotations.index.tolist()\n        self.annotations = self.annotations.loc[\n            list(common(available_annotations, available_features))\n        ]\n        self.frames_dir = frames_dir\n        self.binary_mode = binary_mode\n        Info(f\"created dataset of {len(self)} items\")\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def preprocess(self, features):\n        return features\n\n    # @lru_cache\n    def __getitem__(self, index):\n        row = self.annotations.iloc[index]\n        features = loaddill(\n            self.features_dir / f\"{row.name}.frames.features.tensor\"\n        ).cpu()\n        features = self.preprocess(features)\n        label = row[\"label\"]\n        if self.binary_mode:\n            label = int(label != \"others\")\n        else:\n            label = self.label2id[label]\n        if 0:\n            frames = loaddill(self.frames_dir / f\"{row.name}.frames.tensor\")\n        return {\n            \"features\": features.cpu().detach(),\n            \"targets\": label,\n        }\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\n\nfeatures_dim = 512\nfeatures_dir = root / \"ssbd-frames-features/10fps/resnet18\"\n\n\nannotations_path = root / \"annotations.csv\"\nannotations = pd.read_csv(annotations_path)\nBINARY_MODE = False\n\nds = FeaturesDataset(features_dir, annotations)\nds[0]\n\n\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.15, random_state=11\n)\n\n# trn_df = annotations.loc[\n#     annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n# ]\ntrn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n\nds = FeaturesDataset(features_dir, annotations, binary_mode=BINARY_MODE)\ntrn_ds = FeaturesDataset(features_dir, trn_df, binary_mode=BINARY_MODE)\nval_ds = FeaturesDataset(features_dir, val_df, binary_mode=BINARY_MODE)\nprint(\n    \"train\",\n    collections.Counter([i[\"targets\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"targets\"] for i in track2(val_ds)]),\n)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size: int, dropout=0.1, max_len=512):\n        super(PositionalEncoding, self).__init__()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, hidden_size)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.pow(\n            1e4, -torch.arange(0, hidden_size, 2).float() / hidden_size\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass TransformerEncoderForTimmFeatures(nn.Module):\n    def __init__(\n        self,\n        transformer_layers: int,\n        emb_size: int,\n        max_len: int,\n        features_dim: int,\n        num_classes: int = 4,\n        d_model: int = 512,\n        n_head: int = 8,\n    ):\n        super().__init__()\n        self.positional_encoding = PositionalEncoding(\n            hidden_size=emb_size, max_len=max_len\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer=encoder_layer, num_layers=transformer_layers\n        )\n        self.lin = nn.Sequential(\n            nn.Linear(features_dim, emb_size),\n            nn.ReLU(inplace=True),\n        )\n        self.linear = nn.Linear(d_model, num_classes)\n        self.main_input_name = \"features\"\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.dropout = 0.25\n\n    def forward(self, features, attention_mask, targets=None):\n        Debug(f\"{features=}\")\n        features = self.lin(features)\n        embeddings = self.positional_encoding(features)\n        # embeddings = F.dropout(embeddings, self.dropout)\n        transformer_output = self.transformer_encoder(\n            embeddings.swapaxes(1, 0), src_key_padding_mask=attention_mask.bool()\n        ).swapaxes(1, 0)\n        # transformer_output = F.dropout(transformer_output, self.dropout)\n        pooled_output = transformer_output.mean(dim=1)\n        logits = self.linear(pooled_output)\n        if targets is not None:\n            loss = self.loss_fn(logits, targets)\n        else:\n            loss = -1\n        return {\"loss\": loss, \"logits\": logits}\n\n\ndef collate_fn(batch):\n    seq_len = 128\n    embeddings = [item[\"features\"] for item in batch]\n    Debug(f\"{embeddings=}\")\n    starts = [randint(len(i) - seq_len) if len(i) &gt; seq_len else 0 for i in embeddings]\n    Debug(f\"{starts=}\")\n    embeddings = [\n        e if len(e) &lt; seq_len else e[starts[ix] : starts[ix] + seq_len]\n        for ix, e in enumerate(embeddings)\n    ]\n    Debug(f\"{embeddings=}\")\n    batched_embeddings = pad_sequence(embeddings, batch_first=True).float()\n    Debug(f\"{batched_embeddings=}\")\n    batch_size, seq_len, *_ = batched_embeddings.shape\n    attention_mask = torch.zeros((batch_size, seq_len), dtype=torch.float)\n    Debug(f\"{attention_mask=}\")\n    for i, seq in enumerate(embeddings):\n        attention_mask[i, : len(embeddings)] = 1\n    Debug(f\"{attention_mask=}\")\n    if \"targets\" in batch[0]:\n        Debug(f\"has targets\")\n        labels = [item[\"targets\"] for item in batch]\n        Debug(f\"{labels=}\")\n        return {\n            \"features\": batched_embeddings,\n            \"attention_mask\": attention_mask,\n            \"targets\": torch.Tensor(labels).long(),\n        }\n    else:\n        Debug(f\"not has targets\")\n        return {\n            \"embeddings\": batched_embeddings,\n            \"attention_mask\": attention_mask,\n        }\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nmodel = TransformerEncoderForTimmFeatures(4, features_dim, 128, d_model=features_dim)\nwith debug_mode():\n    b = next(iter(dl))\n    o = model(**b)\nprint(o)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoderForTimmFeatures(\n    4, features_dim, 128, features_dim, d_model=features_dim\n)\nreset_logger()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    id2label = trn_ds.id2label if not BINARY_MODE else [\"others\", \"action\"]\n    pred = np.array([id2label[p] for p in pred])\n    targets = np.array([id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./transformers_model_trained_resnet18\",\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    max_steps=2000,\n    logging_steps=200,\n    save_steps=200,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    eval_accumulation_steps=1,\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 30,  # Adjust this as needed\n    # \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n_ = trainer.predict(val_ds)\n\n\n_ = trainer.predict(val_ds)\n\n\n_ = trainer.predict(trn_ds)\n\n\nbackup_this_notebook(\"07.00_transformer_timm_resnet18.ipynb\")"
  },
  {
    "objectID": "model/transformer_r50.html",
    "href": "model/transformer_r50.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from clip_video_classifier.cli import cli\nfrom functools import lru_cache\nfrom torch_snippets import *\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass FeaturesDataset(Dataset):\n    labels = [\"ArmFlapping\", \"HeadBanging\", \"Spinning\", \"others\"]\n    label2id = {l: ix for ix, l in enumerate(labels)}\n    id2label = {ix: l for l, ix in label2id.items()}\n\n    def __init__(\n        self,\n        features_dir: str,\n        annotations: str,\n        average_features: bool = False,\n        frames_dir: str = None,\n        binary_mode: bool = False,\n    ):\n        self.average_features = average_features\n        self.features_dir = P(features_dir)\n        if isinstance(annotations, (str, P)):\n            self.annotations = pd.read_csv(annotations)\n        else:\n            self.annotations = annotations\n        available_features = [\n            int(stem(f).split(\".\")[0]) for f in self.features_dir.ls()\n        ]\n        available_annotations = self.annotations.index.tolist()\n        self.annotations = self.annotations.loc[\n            list(common(available_annotations, available_features))\n        ]\n        self.frames_dir = frames_dir\n        self.binary_mode = binary_mode\n        Info(f\"created dataset of {len(self)} items\")\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def preprocess(self, features):\n        return features[0].permute(1, 0, 2, 3)\n\n    # @lru_cache\n    def __getitem__(self, index):\n        row = self.annotations.iloc[index]\n        features = loaddill(\n            self.features_dir / f\"{row.name}.frames.features.tensor\"\n        ).cpu()\n        features = self.preprocess(features)\n        label = row[\"label\"]\n        if self.binary_mode:\n            label = int(label != \"others\")\n        else:\n            label = self.label2id[label]\n        if 0:\n            frames = loaddill(self.frames_dir / f\"{row.name}.frames.tensor\")\n        return {\n            \"features\": features.cpu().detach().mean((2, 3)),\n            # \"features\": features.cpu().detach().view(len(features), 1024, -1).contiguous(),\n            # \"features\": features.cpu().detach(),\n            \"targets\": label,\n        }\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nfeatures_dir = root / \"ssbd-frames-features/10fps/slow_r50\"\nannotations_path = root / \"annotations.csv\"\nannotations = pd.read_csv(annotations_path)\nBINARY_MODE = False\n\nds = FeaturesDataset(features_dir, annotations)\nds[0]\n\n\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.35, random_state=11\n)\n\ntrn_df = annotations.loc[\n    annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n]\n# trn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n# val_df = annotations.loc[\n#     annotations.query(\"video in @val_items\").groupby(\"video\")[\"start\"].idxmin()\n# ]\n\nds = FeaturesDataset(features_dir, annotations, binary_mode=BINARY_MODE)\ntrn_ds = FeaturesDataset(features_dir, trn_df, binary_mode=BINARY_MODE)\nval_ds = FeaturesDataset(features_dir, val_df, binary_mode=BINARY_MODE)\n# print(\n#     \"train\",\n#     collections.Counter([i[\"targets\"] for i in track2(trn_ds)]),\n#     \"validation\",\n#     collections.Counter([i[\"targets\"] for i in track2(val_ds)]),\n# )\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size: int, dropout=0.1, max_len=512):\n        super(PositionalEncoding, self).__init__()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, hidden_size)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.pow(\n            1e4, -torch.arange(0, hidden_size, 2).float() / hidden_size\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass TransformerEncoderWithCNN(nn.Module):\n    def __init__(\n        self,\n        transformer_layers: int,\n        emb_size: int,\n        max_len: int,\n        num_classes: int = 4,\n        d_model: int = 512,\n        n_head: int = 8,\n    ):\n        super().__init__()\n        self.positional_encoding = PositionalEncoding(\n            hidden_size=emb_size, max_len=max_len\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer=encoder_layer, num_layers=transformer_layers\n        )\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1024, 32, 5, stride=4, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, 5, stride=4, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.linear = nn.Linear(d_model, num_classes)\n        self.main_input_name = \"features\"\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.dropout = 0.25\n\n    def forward(self, features, attention_mask, targets=None):\n        Debug(f\"{features=}\")\n        # b, ts, c, h, w = features.shape\n        # _features = features.view(b*ts, c, h, w)\n        # Debug(f'{_features=}')\n        # _features = self.cnn(_features)\n        # Debug(f'{_features=}')\n        # *_, c, h, w = _features.shape\n        # features = _features.view(b, ts, c) # h and w are 1\n        # Debug(f'{features=}')\n        embeddings = self.positional_encoding(features)\n        # embeddings = F.dropout(embeddings, self.dropout)\n        transformer_output = self.transformer_encoder(\n            embeddings.swapaxes(1, 0), src_key_padding_mask=attention_mask.bool()\n        ).swapaxes(1, 0)\n        # transformer_output = F.dropout(transformer_output, self.dropout)\n        pooled_output = transformer_output.mean(dim=1)\n        logits = self.linear(pooled_output)\n        if targets is not None:\n            loss = self.loss_fn(logits, targets)\n        else:\n            loss = -1\n        return {\"loss\": loss, \"logits\": logits}\n\n\ndef collate_fn(batch):\n    seq_len = 128\n    embeddings = [item[\"features\"] for item in batch]\n    Debug(f\"{embeddings=}\")\n    starts = [randint(len(i) - seq_len) if len(i) &gt; seq_len else 0 for i in embeddings]\n    Debug(f\"{starts=}\")\n    embeddings = [\n        e if len(e) &lt; seq_len else e[starts[ix] : starts[ix] + seq_len]\n        for ix, e in enumerate(embeddings)\n    ]\n    Debug(f\"{embeddings=}\")\n    batched_embeddings = pad_sequence(embeddings, batch_first=True).float()\n    Debug(f\"{batched_embeddings=}\")\n    batch_size, seq_len, *_ = batched_embeddings.shape\n    attention_mask = torch.zeros((batch_size, seq_len), dtype=torch.float)\n    Debug(f\"{attention_mask=}\")\n    for i, seq in enumerate(embeddings):\n        attention_mask[i, : len(embeddings)] = 1\n    Debug(f\"{attention_mask=}\")\n    if \"targets\" in batch[0]:\n        Debug(f\"has targets\")\n        labels = [item[\"targets\"] for item in batch]\n        Debug(f\"{labels=}\")\n        return {\n            \"features\": batched_embeddings,\n            \"attention_mask\": attention_mask,\n            \"targets\": torch.Tensor(labels).long(),\n        }\n    else:\n        Debug(f\"not has targets\")\n        return {\n            \"embeddings\": batched_embeddings,\n            \"attention_mask\": attention_mask,\n        }\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nmodel = TransformerEncoderWithCNN(4, 1024, 128, d_model=1024)\nwith debug_mode():\n    b = next(iter(dl))\n    o = model(**b)\nprint(o)\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nmodel = TransformerEncoderWithCNN(4, 32, 128, d_model=32)\nwith debug_mode():\n    b = next(iter(dl))\n    o = model(**b)\nprint(o)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoderWithCNN(4, 1024, 128, d_model=1024)\nreset_logger()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    id2label = trn_ds.id2label if not BINARY_MODE else [\"others\", \"action\"]\n    pred = np.array([id2label[p] for p in pred])\n    targets = np.array([id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./transformers_model_trained\",\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=1,\n    max_steps=200,\n    logging_steps=20,\n    save_steps=20,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    eval_accumulation_steps=1,\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 30,  # Adjust this as needed\n    # \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\ntrainer.predict(val_ds)\n\n\nbackup_this_notebook(\"05_transformer_r50.ipynb\")"
  },
  {
    "objectID": "model/infer.html",
    "href": "model/infer.html",
    "title": "Infer using a Transformer",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\nUsage\ninference = Inference(\"/mnt/347832F37832B388/projects/ssbd/cogniable-assignment/artifacts/a/pytorch_model.bin\")\nvideo_path = \"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-raw-videos/v_ArmFlapping_09.mp4\"\ncontent = inference.predict_on_video_path(video_path)\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "model/transformer.html",
    "href": "model/transformer.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nembeddings_folder = root / \"ssbd-embeddings/10fps\"\nframes_folder = root / \"ssbd-frames/10fps\"\nannotations = pd.read_csv(annotations_path)\n# annotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.35, random_state=11\n)\n\ntrn_df = annotations.loc[\n    annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n]\n# trn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n\nds = ClipEmbeddingsDataset(embeddings_folder, annotations)\ntrn_ds = ClipEmbeddingsDataset(embeddings_folder, trn_df)\nval_ds = ClipEmbeddingsDataset(embeddings_folder, val_df)\nprint(\n    \"train\",\n    collections.Counter([i[\"label\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"label\"] for i in track2(val_ds)]),\n)\n\n\ndef collate_fn(batch):\n    seq_len = 128\n    embeddings = [item[\"embeddings\"] for item in batch]\n    starts = [randint(len(i) - seq_len) if len(i) &gt; seq_len else 0 for i in embeddings]\n    embeddings = [\n        e if len(e) &lt; seq_len else e[starts[ix] : starts[ix] + seq_len]\n        for ix, e in enumerate(embeddings)\n    ]\n    labels = [item[\"targets\"] for item in batch]\n    batched_embeddings = pad_sequence(embeddings, batch_first=True).float()\n    batch_size, seq_len, _ = batched_embeddings.shape\n    attention_mask = torch.zeros((batch_size, seq_len), dtype=torch.float)\n    for i, seq in enumerate(embeddings):\n        attention_mask[i, : len(embeddings)] = 1\n\n    return {\n        \"embeddings\": batched_embeddings,\n        \"attention_mask\": attention_mask,\n        \"targets\": torch.Tensor(labels).long(),\n    }\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nb = next(iter(dl))\nmodel = TransformerEncoder(4, 512, 128)\nmodel(**b)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoder(4, 512, 128).cuda()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    pred = np.array([trn_ds.id2label[p] for p in pred])\n    targets = np.array([trn_ds.id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./transformers_model_trained\",\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    max_steps=200,\n    logging_steps=20,\n    save_steps=20,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 30,  # Adjust this as needed\n    \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\ntrainer.predict(val_ds)\n\n\nmodel = TransformerEncoder(4, 512, 128).cuda()\nload_torch_model_weights_to(model, \"saved-models/a/pytorch_model.bin\")\ntraining_args = TrainingArguments(\n    output_dir=\"./linear_model_trained\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    label_names=[\"targets\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n)\n\npredictions = trainer.evaluate(val_ds)\npredictions = trainer.evaluate(ds)\n\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "model/frames_to_embeddings.html",
    "href": "model/frames_to_embeddings.html",
    "title": "Export Embeddings",
    "section": "",
    "text": "Setup the object\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nf2e = Frame2Embeddings()\nUsage for a single set of frames\nframes_folder = root / \"ssbd-frames/10fps\"\nframes_path = frames_folder.ls()[0]\nframes = loaddill(frames_path)\nsubplots(frames)\nf2e(frames_path)\n\nUsage for a folder of frames\nembeddings_folder = root/\"ssbd-embeddings/10fps\"\nf2e(frames_folder, embeddings_folder=embeddings_folder, n=3)\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "data/embeddings_dataset.html",
    "href": "data/embeddings_dataset.html",
    "title": "Download Data",
    "section": "",
    "text": "Usage\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nembeddings_folder = root / \"ssbd-embeddings/5fps\"\nframes_folder = root / \"ssbd-frames/5fps\"\n\nds = ClipEmbeddingsDataset(\n    embeddings_folder, annotations_path, frames_dir=frames_folder\n)\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "00_cli.html",
    "href": "00_cli.html",
    "title": "clip-video-classifier",
    "section": "",
    "text": "from torch_snippets import *\n\n__root = \"../\""
  },
  {
    "objectID": "data/create_annotations.html",
    "href": "data/create_annotations.html",
    "title": "Download Data",
    "section": "",
    "text": "Usage\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nsetup_annotations(root)\n\n# AFTER DOWNLOADING THE VIDEOS\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nsetup_annotations(\n    root,\n    fill_gaps=True,\n    videos_folder=\"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-raw-videos/\",\n)"
  },
  {
    "objectID": "data/video_to_frames.html",
    "href": "data/video_to_frames.html",
    "title": "Download Data",
    "section": "",
    "text": "root = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nannotations = pd.read_csv(annotations_path).rename({\"class\": \"activity\"}, axis=1)\n\nUsage for a single video\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nvideos_folder = root / \"ssbd-raw-videos\"\nframes_folder = root / \"ssbd-frames-5fps\"\n\nannotations = (\n    pd.read_csv(root / \"annotations.csv\")\n    .rename({\"class\": \"activity\"}, axis=1)\n    .query('activity != \"others\"')\n)\n# row = choose(annotations)\nshow(row.to_frame().T)\n\nvideo = videos_folder / f\"{row.video}.mp4\"\nframes = frames_folder / f\"{row.name}.frames.tensor\"\nvideo_to_frames(video, frames, row.start, row.clip_duration, num_frames_per_sec=5)\n\nUsage for all rows in annotations\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nvideos_folder = root / \"ssbd-raw-videos\"\nframes_folder = root / \"ssbd-frames-5fps\"\nnum_frames_per_sec = 5\nexclude_others = True\n\nextract_frames_for_annotations(\n    annotations_path, videos_folder, frames_folder, num_frames_per_sec=5\n)\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "clip-video-classifier",
    "section": "",
    "text": "git clone https://github.com/sizhky/ssbd\ncd ssbd\npip install -e ."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "clip-video-classifier",
    "section": "",
    "text": "git clone https://github.com/sizhky/ssbd\ncd ssbd\npip install -e ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "clip-video-classifier",
    "section": "How to use",
    "text": "How to use\nUse\nclip-video-classifier --help\nclip-video-classifier COMMAND --help\nto see help for all commands\n\nTo train a transformer model on SSBD videos using CLIP as feature extractor\n\nFirst setup your annotations\n\nDATA_DIR=\"/mnt/347832F37832B388/ml-datasets/ssbd\"\nRAW_VIDEO_DIR=\"$DATA_DIR/ssbd-raw-videos\"\nclip-video-classifier setup-annotations $DATA_DIR\nclip-video-classifier download-raw-videos $DATA_DIR/annotations.csv $RAW_VIDEO_DIR\nclip-video-classifier setup-annotations $DATA_DIR --fill-gaps --videos-folder $RAW_VIDEO_DIR\n\nNext extract frames for each video\n\n# change the num-frames-per-sec in the below script if needed\n$ chmod +x scripts/extract_frames.sh\n$ ./extract_frames.sh\n\nNow extract embeddings for each frames.tensor file saved\n\nclip-video-classifier frames-to-embeddings \"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-frames/5fps\" \"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-embeddings/5fps\" \"ViT-B/32\" \"cuda\""
  },
  {
    "objectID": "model/linear.html",
    "href": "model/linear.html",
    "title": "Train Embeddings Using a linear Probe",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\n\nimport torch\nimport torch.multiprocessing as mp\n\nmp.set_start_method(\"spawn\")\n\n\nfrom clip_video_classifier.cli import cli\nfrom clip_video_classifier.data.dataset import ClipEmbeddingsDataset\nfrom torch_snippets import *\n\n\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(512, 64), nn.ReLU(inplace=True), nn.Linear(64, 4)\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.main_input_name = \"input\"\n\n    def forward(self, input, targets):\n        logits = self.model(input)\n        loss = self.loss_fn(logits, targets)\n        return {\"loss\": loss, \"logits\": logits}\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nembeddings_folder = root / \"ssbd-embeddings/10fps\"\nframes_folder = root / \"ssbd-frames/10fps\"\nannotations = pd.read_csv(annotations_path)\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.15, random_state=11\n)\ntrn_df, val_df = (\n    annotations.loc[\n        annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n    ],\n    annotations.query(\"video in @val_items\"),\n)\n\ntrn_ds = ClipEmbeddingsDataset(embeddings_folder, trn_df, frames_dir=frames_folder)\nval_ds = ClipEmbeddingsDataset(embeddings_folder, val_df, frames_dir=frames_folder)\nprint(\n    \"train\",\n    collections.Counter([i[\"label\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"label\"] for i in track2(val_ds)]),\n)\n\n\ndef collate_fn(batch):\n    output = {}\n    output[\"input\"] = torch.stack([i[\"input\"] for i in batch]).to(torch.float)\n    output[\"targets\"] = torch.tensor([i[\"targets\"] for i in batch])\n    return output\n\ndl = DataLoader(trn_ds, shuffle=True, batch_size=3, collate_fn=collate_fn)\nmodel = LinearModel()\ni = next(iter(dl))\nmodel(**i)\n\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = LinearModel().cuda()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    pred = np.array([trn_ds.id2label[p] for p in pred])\n    targets = np.array([trn_ds.id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./linear_model_trained\",\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    num_train_epochs=1500,\n    logging_steps=1500,\n    save_steps=200,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n\ntrainer.predict(trn_ds)\n\n\ntrainer.predict(val_ds)\n\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "model/transformer_clip.html",
    "href": "model/transformer_clip.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nembeddings_folder = root / \"ssbd-embeddings/10fps\"\nframes_folder = root / \"ssbd-frames/10fps\"\nSEQUENCE_LENGTH = 512\n\nannotations = pd.read_csv(annotations_path)\nBINARY_MODE = False\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.15, random_state=11\n)\n\ntrn_df = annotations.loc[\n    annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n]\n# trn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n\nds = ClipEmbeddingsDataset(embeddings_folder, annotations, binary_mode=BINARY_MODE)\ntrn_ds = ClipEmbeddingsDataset(embeddings_folder, trn_df, binary_mode=BINARY_MODE)\nval_ds = ClipEmbeddingsDataset(embeddings_folder, val_df, binary_mode=BINARY_MODE)\nprint(\n    \"train\",\n    collections.Counter([i[\"label\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"label\"] for i in track2(val_ds)]),\n)\n\ndl = DataLoader(\n    trn_ds, batch_size=3, shuffle=True, collate_fn=lambda b: collate_fn(b, 128)\n)\nb = next(iter(dl))\nprint(b)\nmodel = TransformerEncoder(4, 512, 128)\nmodel(**b)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoder(4, 512, SEQUENCE_LENGTH).cuda()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    id2label = trn_ds.id2label if not BINARY_MODE else [\"others\", \"action\"]\n    pred = np.array([id2label[p] for p in pred])\n    targets = np.array([id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\noutput_dir = f\"./transformers_clip_512_seq_length_{rand()}\"\nInfo(f\"Saving models at {output_dir}\")\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    max_steps=200,\n    logging_steps=20,\n    save_steps=20,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 15,  # Adjust this as needed\n    # \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=lambda b: collate_fn(b, SEQUENCE_LENGTH),\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n\n_ = trainer.predict(val_ds)\n\n\n_ = trainer.predict(ds)\n\n\nBINARY_MODE = False\n\nmodel = TransformerEncoder(4, 512, 128).cuda()\nload_torch_model_weights_to(model, \"../../artifacts/a/pytorch_model.bin\")\nmodel.eval()\n\ntraining_args = TrainingArguments(\n    output_dir=\"/tmp\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    label_names=[\"targets\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=lambda b: collate_fn(b, 128),\n    compute_metrics=compute_metrics,\n)\n\npredictions = trainer.evaluate(val_ds)\npredictions = trainer.evaluate(ds)\n\n\nbackup_this_notebook(\"02_transformer_clip.ipynb\")\n\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "model/slowr50_feature_extractor.html",
    "href": "model/slowr50_feature_extractor.html",
    "title": "Infer using a Transformer",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\n\nfrom torchvision.transforms import Compose, Lambda\nfrom torchvision.transforms._transforms_video import (\n    CenterCropVideo,\n    NormalizeVideo,\n)\n\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    ShortSideScale,\n    UniformTemporalSubsample,\n    UniformCropVideo,\n)\nimport pandas as pd\n\n\nside_size = 256\nmean = [0.45, 0.45, 0.45]\nstd = [0.225, 0.225, 0.225]\ncrop_size = 256\nnum_frames = 64\n# sampling_rate = 2\nframes_per_second = 30\nalpha = 4\n\n# model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\ntransform = ApplyTransformToKey(\n    key=\"video\",\n    transform=Compose(\n        [\n            UniformTemporalSubsample(num_frames),\n            Lambda(lambda x: x / 255.0),\n            NormalizeVideo(mean, std),\n            ShortSideScale(size=side_size),\n            CenterCropVideo(crop_size=(crop_size, crop_size)),\n        ]\n    ),\n)\n\n\nfrom torch_snippets import *\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd\")\nannotations = pd.read_csv(f\"{root}/annotations.csv\")\n\n\nside_size = 256\nmean = [0.45, 0.45, 0.45]\nstd = [0.225, 0.225, 0.225]\ncrop_size = 256\nnum_frames = 32\nsampling_rate = 2\nframes_per_second = 30\nalpha = 4\n\nmodel = torch.hub.load(\"facebookresearch/pytorchvideo\", \"slow_r50\", pretrained=True)\n\ntransform = lambda num_frames=num_frames: ApplyTransformToKey(\n    key=\"video\",\n    transform=Compose(\n        [\n            UniformTemporalSubsample(num_frames),\n            Lambda(lambda x: x / 255.0),\n            NormalizeVideo(mean, std),\n            ShortSideScale(size=side_size),\n            CenterCropVideo(crop_size=(crop_size, crop_size)),\n        ]\n    ),\n)\n\n\nrow = choose(annotations.query(\"start == 3 and end == 7\"))\nshow(row.to_frame().T)\n\nModel from Hub\nmodel = torch.hub.load(\n    \"facebookresearch/pytorchvideo\", \"slow_r50\", pretrained=True\n).cuda()\n\nimport json\nwith open(\"kinetics_classnames.json\", \"r\") as f:\n    kinetics_classnames = json.load(f)\nkinetics_id_to_classname = {}\nfor k, v in kinetics_classnames.items():\n    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\nraw_videos_folder = root / \"ssbd-raw-videos\"\nvideo = EncodedVideo.from_path(raw_videos_folder / f\"{row.video}.mp4\")\nvideo_data = video.get_clip(start_sec=row.start, end_sec=row.end)\nInfo(video_data)\nvideo_data = transform()(video_data)\n# num_frames = (row.end - row.start) * 10\n# video_data = transform_num_frames(num_frames)(video_data)\nInfo(video_data)\ninputs = video_data[\"video\"][None]\nInfo(inputs)\n# Generate top 5 predictions\n\nwith torch.no_grad():\n    preds = model(inputs.cuda())\n\npreds = torch.nn.functional.softmax(preds, dim=-1)\nprint(preds)\npred_class_ids = preds.topk(k=5).indices\n\npred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_class_ids[0]]\nprint(\"Predicted labels: %s\" % \", \".join(pred_class_names))\n\nframes_folder = root / \"ssbd-frames/10fps\"\n\nmean_transform = ApplyTransformToKey(\n    key=\"video\",\n    transform=Compose(\n        [\n            UniformTemporalSubsample(num_frames),\n            NormalizeVideo(mean, std),\n            ShortSideScale(size=side_size),\n            CenterCropVideo(crop_size=(crop_size, crop_size)),\n        ]\n    ),\n)\n\nfor frames_path in Glob(frames_folder):\n    frames = {\"video\": loaddill(frames_path).permute(1, 0, 2, 3)}\n    # frames = {\"video\": loaddill(frames_folder / \"198.frames.tensor\").permute(1, 0, 2, 3)}\n    Info(frames)\n    frames = mean_transform(frames)[\"video\"][None]\n    Info(frames)\n\n    with torch.no_grad():\n        preds = model(frames.cuda())\n\n    preds = torch.nn.functional.softmax(preds, dim=-1)\n    print(preds)\n    pred_class_ids = preds.topk(k=5).indices\n\n    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_class_ids[0]]\n    print(\"Predicted labels: %s\" % \", \".join(pred_class_names))\n\nframes_folder = root / \"ssbd-frames/10fps\"\n\nmean_transform = ApplyTransformToKey(\n    key=\"video\",\n    transform=Compose(\n        [\n            NormalizeVideo(mean, std),\n        ]\n    ),\n)\n\nfeature_extractor = nn.Sequential(*model.blocks[:4]).cpu()\nfeatures_folder = root / \"ssbd-frames-features/10fps/slow_r50/\"\nmakedir(features_folder)\n\nfor frames_path in (tracker := track2(Glob(frames_folder))):\n    item = stem(frames_path)\n    if item in [\"477.frames\", \"407.frames\"]:\n        continue\n    to = features_folder / f\"{item}.features.tensor\"\n    if exists(to):\n        continue\n    frames = loaddill(frames_path).permute(1, 0, 2, 3)\n    frames = {\"video\": frames}\n    frames = mean_transform(frames)[\"video\"][None]\n    tracker.send(f\"processing {item} @ {frames}\")\n    with torch.no_grad():\n        try:\n            preds = feature_extractor(frames.cpu()).cpu()\n            dumpdill(preds, to, silent=True)\n        except Exception as e:\n            Warn(f\"{e} @ {item}\")\n\n\nmodel"
  },
  {
    "objectID": "model/timm_feature_extractor.html",
    "href": "model/timm_feature_extractor.html",
    "title": "Infer using a Transformer",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\nUsage\nfrom torch_snippets import *\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd\")\nannotations = pd.read_csv(f\"{root}/annotations.csv\")\n\nMODELS = [\"vgg19\", \"resnet18\", \"resnet50\", \"densenet121\"]\nfor model in MODELS:\n    frames_folder = root / \"ssbd-frames/10fps\"\n    features_folder = root / f\"ssbd-frames-features/10fps/{model}/\"\n    makedir(features_folder)\n    extract_features_for_all_frames(model, frames_folder, features_folder, \"cuda\")"
  },
  {
    "objectID": "model/07.01_transformer_timm_vgg19.html",
    "href": "model/07.01_transformer_timm_vgg19.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from clip_video_classifier.cli import cli\nfrom functools import lru_cache\nfrom torch_snippets import *\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass FeaturesDataset(Dataset):\n    labels = [\"ArmFlapping\", \"HeadBanging\", \"Spinning\", \"others\"]\n    label2id = {l: ix for ix, l in enumerate(labels)}\n    id2label = {ix: l for l, ix in label2id.items()}\n\n    def __init__(\n        self,\n        features_dir: str,\n        annotations: str,\n        average_features: bool = False,\n        frames_dir: str = None,\n        binary_mode: bool = False,\n    ):\n        self.average_features = average_features\n        self.features_dir = P(features_dir)\n        if isinstance(annotations, (str, P)):\n            self.annotations = pd.read_csv(annotations)\n        else:\n            self.annotations = annotations\n        available_features = [\n            int(stem(f).split(\".\")[0]) for f in self.features_dir.ls()\n        ]\n        available_annotations = self.annotations.index.tolist()\n        self.annotations = self.annotations.loc[\n            list(common(available_annotations, available_features))\n        ]\n        self.frames_dir = frames_dir\n        self.binary_mode = binary_mode\n        Info(f\"created dataset of {len(self)} items\")\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def preprocess(self, features):\n        return features\n\n    # @lru_cache\n    def __getitem__(self, index):\n        row = self.annotations.iloc[index]\n        features = loaddill(\n            self.features_dir / f\"{row.name}.frames.features.tensor\"\n        ).cpu()\n        features = self.preprocess(features)\n        label = row[\"label\"]\n        if self.binary_mode:\n            label = int(label != \"others\")\n        else:\n            label = self.label2id[label]\n        if 0:\n            frames = loaddill(self.frames_dir / f\"{row.name}.frames.tensor\")\n        return {\n            \"features\": features.cpu().detach(),\n            \"targets\": label,\n        }\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\n\nfeatures_dim = 512\nfeatures_dir = root / \"ssbd-frames-features/10fps/vgg19\"\nmodel_output_dir = \"./transformers_model_trained_vgg19\"\n\nannotations_path = root / \"annotations.csv\"\nannotations = pd.read_csv(annotations_path)\nBINARY_MODE = False\n\nds = FeaturesDataset(features_dir, annotations)\nds[0]\n\n\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.15, random_state=11\n)\n\n# trn_df = annotations.loc[\n#     annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n# ]\ntrn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n\nds = FeaturesDataset(features_dir, annotations, binary_mode=BINARY_MODE)\ntrn_ds = FeaturesDataset(features_dir, trn_df, binary_mode=BINARY_MODE)\nval_ds = FeaturesDataset(features_dir, val_df, binary_mode=BINARY_MODE)\nprint(\n    \"train\",\n    collections.Counter([i[\"targets\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"targets\"] for i in track2(val_ds)]),\n)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size: int, dropout=0.1, max_len=512):\n        super(PositionalEncoding, self).__init__()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, hidden_size)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.pow(\n            1e4, -torch.arange(0, hidden_size, 2).float() / hidden_size\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass TransformerEncoderForTimmFeatures(nn.Module):\n    def __init__(\n        self,\n        transformer_layers: int,\n        emb_size: int,\n        max_len: int,\n        features_dim: int,\n        num_classes: int = 4,\n        d_model: int = 512,\n        n_head: int = 8,\n    ):\n        super().__init__()\n        self.positional_encoding = PositionalEncoding(\n            hidden_size=emb_size, max_len=max_len\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer=encoder_layer, num_layers=transformer_layers\n        )\n        self.lin = nn.Sequential(\n            nn.Linear(features_dim, emb_size),\n            nn.ReLU(inplace=True),\n        )\n        self.linear = nn.Linear(d_model, num_classes)\n        self.main_input_name = \"features\"\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.dropout = 0.25\n\n    def forward(self, features, attention_mask, targets=None):\n        Debug(f\"{features=}\")\n        features = self.lin(features)\n        embeddings = self.positional_encoding(features)\n        # embeddings = F.dropout(embeddings, self.dropout)\n        transformer_output = self.transformer_encoder(\n            embeddings.swapaxes(1, 0), src_key_padding_mask=attention_mask.bool()\n        ).swapaxes(1, 0)\n        # transformer_output = F.dropout(transformer_output, self.dropout)\n        pooled_output = transformer_output.mean(dim=1)\n        logits = self.linear(pooled_output)\n        if targets is not None:\n            loss = self.loss_fn(logits, targets)\n        else:\n            loss = -1\n        return {\"loss\": loss, \"logits\": logits}\n\n\ndef collate_fn(batch):\n    seq_len = 128\n    embeddings = [item[\"features\"] for item in batch]\n    Debug(f\"{embeddings=}\")\n    starts = [randint(len(i) - seq_len) if len(i) &gt; seq_len else 0 for i in embeddings]\n    Debug(f\"{starts=}\")\n    embeddings = [\n        e if len(e) &lt; seq_len else e[starts[ix] : starts[ix] + seq_len]\n        for ix, e in enumerate(embeddings)\n    ]\n    Debug(f\"{embeddings=}\")\n    batched_embeddings = pad_sequence(embeddings, batch_first=True).float()\n    Debug(f\"{batched_embeddings=}\")\n    batch_size, seq_len, *_ = batched_embeddings.shape\n    attention_mask = torch.zeros((batch_size, seq_len), dtype=torch.float)\n    Debug(f\"{attention_mask=}\")\n    for i, seq in enumerate(embeddings):\n        attention_mask[i, : len(embeddings)] = 1\n    Debug(f\"{attention_mask=}\")\n    if \"targets\" in batch[0]:\n        Debug(f\"has targets\")\n        labels = [item[\"targets\"] for item in batch]\n        Debug(f\"{labels=}\")\n        return {\n            \"features\": batched_embeddings[..., ::8],\n            \"attention_mask\": attention_mask,\n            \"targets\": torch.Tensor(labels).long(),\n        }\n    else:\n        Debug(f\"not has targets\")\n        return {\n            \"embeddings\": batched_embeddings[..., ::8],\n            \"attention_mask\": attention_mask,\n        }\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nmodel = TransformerEncoderForTimmFeatures(4, features_dim, 128, d_model=features_dim)\nwith debug_mode():\n    b = next(iter(dl))\n    o = model(**b)\nprint(o)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoderForTimmFeatures(\n    4, features_dim, 128, features_dim, d_model=features_dim\n)\nreset_logger()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    id2label = trn_ds.id2label if not BINARY_MODE else [\"others\", \"action\"]\n    pred = np.array([id2label[p] for p in pred])\n    targets = np.array([id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=model_output_dir,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    max_steps=512,\n    logging_steps=64,\n    save_steps=64,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    eval_accumulation_steps=1,\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 16,  # Adjust this as needed\n    # \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n\n_ = trainer.predict(val_ds)\n\n\n_ = trainer.predict(trn_ds)\n\n\nbackup_this_notebook(\"07.01_transformer_timm_vgg19.ipynb\")"
  },
  {
    "objectID": "model/07.03_transformer_timm_densenet121.html",
    "href": "model/07.03_transformer_timm_densenet121.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from clip_video_classifier.cli import cli\nfrom functools import lru_cache\nfrom torch_snippets import *\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass FeaturesDataset(Dataset):\n    labels = [\"ArmFlapping\", \"HeadBanging\", \"Spinning\", \"others\"]\n    label2id = {l: ix for ix, l in enumerate(labels)}\n    id2label = {ix: l for l, ix in label2id.items()}\n\n    def __init__(\n        self,\n        features_dir: str,\n        annotations: str,\n        average_features: bool = False,\n        frames_dir: str = None,\n        binary_mode: bool = False,\n    ):\n        self.average_features = average_features\n        self.features_dir = P(features_dir)\n        if isinstance(annotations, (str, P)):\n            self.annotations = pd.read_csv(annotations)\n        else:\n            self.annotations = annotations\n        available_features = [\n            int(stem(f).split(\".\")[0]) for f in self.features_dir.ls()\n        ]\n        available_annotations = self.annotations.index.tolist()\n        self.annotations = self.annotations.loc[\n            list(common(available_annotations, available_features))\n        ]\n        self.frames_dir = frames_dir\n        self.binary_mode = binary_mode\n        Info(f\"created dataset of {len(self)} items\")\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def preprocess(self, features):\n        return features\n\n    # @lru_cache\n    def __getitem__(self, index):\n        row = self.annotations.iloc[index]\n        features = loaddill(\n            self.features_dir / f\"{row.name}.frames.features.tensor\"\n        ).cpu()\n        features = self.preprocess(features)\n        label = row[\"label\"]\n        if self.binary_mode:\n            label = int(label != \"others\")\n        else:\n            label = self.label2id[label]\n        if 0:\n            frames = loaddill(self.frames_dir / f\"{row.name}.frames.tensor\")\n        return {\n            \"features\": features.cpu().detach(),\n            \"targets\": label,\n        }\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\n\nfeatures_dim = 1024\nfeatures_dir = root / \"ssbd-frames-features/10fps/densenet121\"\nmodel_output_dir = f\"./transformers_model_trained_densenet121_{rand()}\"\n\nannotations_path = root / \"annotations.csv\"\nannotations = pd.read_csv(annotations_path)\nBINARY_MODE = False\n\nds = FeaturesDataset(features_dir, annotations)\nds[0]\n\n\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.15, random_state=11\n)\n\n# trn_df = annotations.loc[\n#     annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n# ]\ntrn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n\nds = FeaturesDataset(features_dir, annotations, binary_mode=BINARY_MODE)\ntrn_ds = FeaturesDataset(features_dir, trn_df, binary_mode=BINARY_MODE)\nval_ds = FeaturesDataset(features_dir, val_df, binary_mode=BINARY_MODE)\nprint(\n    \"train\",\n    collections.Counter([i[\"targets\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"targets\"] for i in track2(val_ds)]),\n)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size: int, dropout=0.1, max_len=512):\n        super(PositionalEncoding, self).__init__()\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, hidden_size)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.pow(\n            1e4, -torch.arange(0, hidden_size, 2).float() / hidden_size\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = x + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass TransformerEncoderForTimmFeatures(nn.Module):\n    def __init__(\n        self,\n        transformer_layers: int,\n        emb_size: int,\n        max_len: int,\n        features_dim: int,\n        num_classes: int = 4,\n        d_model: int = 512,\n        n_head: int = 8,\n    ):\n        super().__init__()\n        self.positional_encoding = PositionalEncoding(\n            hidden_size=emb_size, max_len=max_len\n        )\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer=encoder_layer, num_layers=transformer_layers\n        )\n        self.lin = nn.Sequential(\n            nn.Linear(features_dim, emb_size),\n            nn.ReLU(inplace=True),\n        )\n        self.linear = nn.Linear(d_model, num_classes)\n        self.main_input_name = \"features\"\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.dropout = 0.25\n\n    def forward(self, features, attention_mask, targets=None):\n        Debug(f\"{features=}\")\n        features = self.lin(features)\n        embeddings = self.positional_encoding(features)\n        # embeddings = F.dropout(embeddings, self.dropout)\n        transformer_output = self.transformer_encoder(\n            embeddings.swapaxes(1, 0), src_key_padding_mask=attention_mask.bool()\n        ).swapaxes(1, 0)\n        # transformer_output = F.dropout(transformer_output, self.dropout)\n        pooled_output = transformer_output.mean(dim=1)\n        logits = self.linear(pooled_output)\n        if targets is not None:\n            loss = self.loss_fn(logits, targets)\n        else:\n            loss = -1\n        return {\"loss\": loss, \"logits\": logits}\n\n\ndef collate_fn(batch):\n    seq_len = 128\n    embeddings = [item[\"features\"] for item in batch]\n    Debug(f\"{embeddings=}\")\n    starts = [randint(len(i) - seq_len) if len(i) &gt; seq_len else 0 for i in embeddings]\n    Debug(f\"{starts=}\")\n    embeddings = [\n        e if len(e) &lt; seq_len else e[starts[ix] : starts[ix] + seq_len]\n        for ix, e in enumerate(embeddings)\n    ]\n    Debug(f\"{embeddings=}\")\n    batched_embeddings = pad_sequence(embeddings, batch_first=True).float()\n    Debug(f\"{batched_embeddings=}\")\n    batch_size, seq_len, *_ = batched_embeddings.shape\n    attention_mask = torch.zeros((batch_size, seq_len), dtype=torch.float)\n    Debug(f\"{attention_mask=}\")\n    for i, seq in enumerate(embeddings):\n        attention_mask[i, : len(embeddings)] = 1\n    Debug(f\"{attention_mask=}\")\n    if \"targets\" in batch[0]:\n        Debug(f\"has targets\")\n        labels = [item[\"targets\"] for item in batch]\n        Debug(f\"{labels=}\")\n        return {\n            \"features\": batched_embeddings,\n            \"attention_mask\": attention_mask,\n            \"targets\": torch.Tensor(labels).long(),\n        }\n    else:\n        Debug(f\"not has targets\")\n        return {\n            \"embeddings\": batched_embeddings,\n            \"attention_mask\": attention_mask,\n        }\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nmodel = TransformerEncoderForTimmFeatures(4, features_dim, 128, d_model=features_dim)\nwith debug_mode():\n    b = next(iter(dl))\n    o = model(**b)\nprint(o)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoderForTimmFeatures(\n    4, features_dim, 512, features_dim, d_model=features_dim\n)\nreset_logger()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    id2label = trn_ds.id2label if not BINARY_MODE else [\"others\", \"action\"]\n    pred = np.array([id2label[p] for p in pred])\n    targets = np.array([id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\nInfo(f\"Models saved at {model_output_dir}\")\ntraining_args = TrainingArguments(\n    output_dir=model_output_dir,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    max_steps=2000,\n    logging_steps=200,\n    save_steps=200,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    eval_accumulation_steps=1,\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 30,  # Adjust this as needed\n    # \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n\n_ = trainer.predict(val_ds)\n\n\n_ = trainer.predict(trn_ds)\n\n\nbackup_this_notebook(\"07.03_transformer_timm_densenet121.ipynb\")"
  },
  {
    "objectID": "model/backups/02_transformer_clip/changelog.html#transformer_clip__0001",
    "href": "model/backups/02_transformer_clip/changelog.html#transformer_clip__0001",
    "title": "clip-video-classifier",
    "section": "02_transformer_clip__0001",
    "text": "02_transformer_clip__0001\nwith AUCROC curve"
  },
  {
    "objectID": "model/backups/07.00_transformer_timm_resnet18/changelog.html#transformer_timm_resnet18__0001",
    "href": "model/backups/07.00_transformer_timm_resnet18/changelog.html#transformer_timm_resnet18__0001",
    "title": "clip-video-classifier",
    "section": "07.00_transformer_timm_resnet18__0001",
    "text": "07.00_transformer_timm_resnet18__0001"
  },
  {
    "objectID": "model/backups/07.00_transformer_timm_resnet18/changelog.html#transformer_timm_resnet18__0001-1",
    "href": "model/backups/07.00_transformer_timm_resnet18/changelog.html#transformer_timm_resnet18__0001-1",
    "title": "clip-video-classifier",
    "section": "07.00_transformer_timm_resnet18__0001",
    "text": "07.00_transformer_timm_resnet18__0001"
  }
]