[
  {
    "objectID": "model/backups/02_transformer/changelog.html#transformer__0000-1",
    "href": "model/backups/02_transformer/changelog.html#transformer__0000-1",
    "title": "clip-video-classifier",
    "section": "02_transformer__0000",
    "text": "02_transformer__0000"
  },
  {
    "objectID": "model/transformer.html",
    "href": "model/transformer.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nembeddings_folder = root / \"ssbd-embeddings/10fps\"\nframes_folder = root / \"ssbd-frames/10fps\"\nannotations = pd.read_csv(annotations_path)\nBINARY_MODE = False\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.35, random_state=11\n)\n\ntrn_df = annotations.loc[annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()]\n# trn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n\nds = ClipEmbeddingsDataset(embeddings_folder, annotations, binary_mode=BINARY_MODE)\ntrn_ds = ClipEmbeddingsDataset(embeddings_folder, trn_df, binary_mode=BINARY_MODE)\nval_ds = ClipEmbeddingsDataset(embeddings_folder, val_df, binary_mode=BINARY_MODE)\nprint(\n    \"train\",\n    collections.Counter([i[\"label\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"label\"] for i in track2(val_ds)]),\n)\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nb = next(iter(dl))\nmodel = TransformerEncoder(4, 512, 128)\nmodel(**b)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoder(4, 512, 128).cuda()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    id2label = trn_ds.id2label if not BINARY_MODE else [\"others\", \"action\"]\n    pred = np.array([id2label[p] for p in pred])\n    targets = np.array([id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./transformers_model_trained\",\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    max_steps=2000,\n    logging_steps=200,\n    save_steps=200,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 30,  # Adjust this as needed\n    # \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\n# trainer.train()\n# trainer.predict(val_ds)\n\n\nBINARY_MODE = False\n\nmodel = TransformerEncoder(4, 512, 128).cuda()\nload_torch_model_weights_to(model, \"saved-models/a/pytorch_model.bin\")\nmodel.eval()\n\ntraining_args = TrainingArguments(\n    output_dir=\"./linear_model_trained\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    label_names=[\"targets\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n)\n\npredictions = trainer.evaluate(val_ds)\npredictions = trainer.evaluate(ds)\n\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "model/linear.html",
    "href": "model/linear.html",
    "title": "Train Embeddings Using a linear Probe",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\n\nimport torch\nimport torch.multiprocessing as mp\n\nmp.set_start_method(\"spawn\")\n\n\nfrom clip_video_classifier.cli import cli\nfrom clip_video_classifier.data.dataset import ClipEmbeddingsDataset\nfrom torch_snippets import *\n\n\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(512, 64), nn.ReLU(inplace=True), nn.Linear(64, 4)\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.main_input_name = \"input\"\n\n    def forward(self, input, targets):\n        logits = self.model(input)\n        loss = self.loss_fn(logits, targets)\n        return {\"loss\": loss, \"logits\": logits}\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nembeddings_folder = root / \"ssbd-embeddings/10fps\"\nframes_folder = root / \"ssbd-frames/10fps\"\nannotations = pd.read_csv(annotations_path)\nannotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.15, random_state=11\n)\ntrn_df, val_df = (\n    annotations.loc[\n        annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()\n    ],\n    annotations.query(\"video in @val_items\"),\n)\n\ntrn_ds = ClipEmbeddingsDataset(embeddings_folder, trn_df, frames_dir=frames_folder)\nval_ds = ClipEmbeddingsDataset(embeddings_folder, val_df, frames_dir=frames_folder)\nprint(\n    \"train\",\n    collections.Counter([i[\"label\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"label\"] for i in track2(val_ds)]),\n)\n\n\ndef collate_fn(batch):\n    output = {}\n    output[\"input\"] = torch.stack([i[\"input\"] for i in batch]).to(torch.float)\n    output[\"targets\"] = torch.tensor([i[\"targets\"] for i in batch])\n    return output\n\ndl = DataLoader(trn_ds, shuffle=True, batch_size=3, collate_fn=collate_fn)\nmodel = LinearModel()\ni = next(iter(dl))\nmodel(**i)\n\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = LinearModel().cuda()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    pred = np.array([trn_ds.id2label[p] for p in pred])\n    targets = np.array([trn_ds.id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./linear_model_trained\",\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    num_train_epochs=1500,\n    logging_steps=1500,\n    save_steps=200,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\n\ntrainer.predict(trn_ds)\n\n\ntrainer.predict(val_ds)\n\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "clip-video-classifier",
    "section": "",
    "text": "git clone https://github.com/sizhky/ssbd\ncd ssbd\npip install -e ."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "clip-video-classifier",
    "section": "",
    "text": "git clone https://github.com/sizhky/ssbd\ncd ssbd\npip install -e ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "clip-video-classifier",
    "section": "How to use",
    "text": "How to use\nUse\nclip-video-classifier --help\nclip-video-classifier COMMAND --help\nto see help for all commands\n\nTo train a transformer model on SSBD videos using CLIP as feature extractor\n\nFirst setup your annotations\n\nDATA_DIR=\"/mnt/347832F37832B388/ml-datasets/ssbd\"\nRAW_VIDEO_DIR=\"$DATA_DIR/ssbd-raw-videos\"\nclip-video-classifier setup-annotations $DATA_DIR\nclip-video-classifier download-raw-videos $DATA_DIR/annotations.csv $RAW_VIDEO_DIR\nclip-video-classifier setup-annotations $DATA_DIR --fill-gaps --videos-folder $RAW_VIDEO_DIR\n\nNext extract frames for each video\n\n# change the num-frames-per-sec in the below script if needed\n$ chmod +x scripts/extract_frames.sh\n$ ./extract_frames.sh\n\nNow extract embeddings for each frames.tensor file saved\n\nclip-video-classifier frames-to-embeddings \"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-frames/5fps\" \"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-embeddings/5fps\" \"ViT-B/32\" \"cuda\""
  },
  {
    "objectID": "data/video_to_frames.html",
    "href": "data/video_to_frames.html",
    "title": "Download Data",
    "section": "",
    "text": "root = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nannotations = pd.read_csv(annotations_path).rename({\"class\": \"activity\"}, axis=1)\n\nUsage for a single video\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nvideos_folder = root / \"ssbd-raw-videos\"\nframes_folder = root / \"ssbd-frames-5fps\"\n\nannotations = (\n    pd.read_csv(root / \"annotations.csv\")\n    .rename({\"class\": \"activity\"}, axis=1)\n    .query('activity != \"others\"')\n)\n# row = choose(annotations)\nshow(row.to_frame().T)\n\nvideo = videos_folder / f\"{row.video}.mp4\"\nframes = frames_folder / f\"{row.name}.frames.tensor\"\nvideo_to_frames(video, frames, row.start, row.clip_duration, num_frames_per_sec=5)\n\nUsage for all rows in annotations\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nvideos_folder = root / \"ssbd-raw-videos\"\nframes_folder = root / \"ssbd-frames-5fps\"\nnum_frames_per_sec = 5\nexclude_others = True\n\nextract_frames_for_annotations(\n    annotations_path, videos_folder, frames_folder, num_frames_per_sec=5\n)\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "data/create_annotations.html",
    "href": "data/create_annotations.html",
    "title": "Download Data",
    "section": "",
    "text": "Usage\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nsetup_annotations(root)\n\n# AFTER DOWNLOADING THE VIDEOS\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nsetup_annotations(\n    root,\n    fill_gaps=True,\n    videos_folder=\"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-raw-videos/\",\n)"
  },
  {
    "objectID": "00_cli.html",
    "href": "00_cli.html",
    "title": "clip-video-classifier",
    "section": "",
    "text": "from torch_snippets import *\n\n__root = \"../\""
  },
  {
    "objectID": "data/download_raw_videos.html",
    "href": "data/download_raw_videos.html",
    "title": "Download Data",
    "section": "",
    "text": "raw_videos_folder = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-raw-videos\")"
  },
  {
    "objectID": "data/embeddings_dataset.html",
    "href": "data/embeddings_dataset.html",
    "title": "Download Data",
    "section": "",
    "text": "Usage\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nembeddings_folder = root / \"ssbd-embeddings/5fps\"\nframes_folder = root / \"ssbd-frames/5fps\"\n\nds = ClipEmbeddingsDataset(\n    embeddings_folder, annotations_path, frames_dir=frames_folder\n)\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "model/frames_to_embeddings.html",
    "href": "model/frames_to_embeddings.html",
    "title": "Export Embeddings",
    "section": "",
    "text": "Setup the object\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nf2e = Frame2Embeddings()\nUsage for a single set of frames\nframes_folder = root / \"ssbd-frames/10fps\"\nframes_path = frames_folder.ls()[0]\nframes = loaddill(frames_path)\nsubplots(frames)\nf2e(frames_path)\n\nUsage for a folder of frames\nembeddings_folder = root/\"ssbd-embeddings/10fps\"\nf2e(frames_folder, embeddings_folder=embeddings_folder, n=3)\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "model/transformer-copy1.html",
    "href": "model/transformer-copy1.html",
    "title": "Train Embeddings Using a Transformer",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\n\nfrom clip_video_classifier.cli import cli\nfrom clip_video_classifier.data.dataset import ClipEmbeddingsDataset\nfrom torch_snippets import *\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nfrom sklearn.model_selection import train_test_split\nimport collections\n\nroot = P(\"/mnt/347832F37832B388/ml-datasets/ssbd/\")\nannotations_path = root / \"annotations.csv\"\nembeddings_folder = root / \"ssbd-embeddings/10fps\"\nframes_folder = root / \"ssbd-frames/10fps\"\nannotations = pd.read_csv(annotations_path)\nBINARY_MODE = True\n# annotations = annotations.query('label != \"others\"')\ntrn_items, val_items = train_test_split(\n    annotations.video.unique(), test_size=0.35, random_state=11\n)\n\n# trn_df = annotations.loc[annotations.query(\"video in @trn_items\").groupby(\"video\")[\"start\"].idxmin()]\ntrn_df = annotations.query(\"video in @trn_items\")\nval_df = annotations.query(\"video in @val_items\")\n\n\nds = ClipEmbeddingsDataset(embeddings_folder, annotations, binary_mode=BINARY_MODE)\ntrn_ds = ClipEmbeddingsDataset(embeddings_folder, trn_df, binary_mode=BINARY_MODE)\nval_ds = ClipEmbeddingsDataset(embeddings_folder, val_df, binary_mode=BINARY_MODE)\nprint(\n    \"train\",\n    collections.Counter([i[\"label\"] for i in track2(trn_ds)]),\n    \"validation\",\n    collections.Counter([i[\"label\"] for i in track2(val_ds)]),\n)\n\n\ndef collate_fn(batch):\n    seq_len = 128\n    embeddings = [item[\"embeddings\"] for item in batch]\n    starts = [randint(len(i) - seq_len) if len(i) &gt; seq_len else 0 for i in embeddings]\n    embeddings = [\n        e if len(e) &lt; seq_len else e[starts[ix] : starts[ix] + seq_len]\n        for ix, e in enumerate(embeddings)\n    ]\n    labels = [item[\"targets\"] for item in batch]\n    batched_embeddings = pad_sequence(embeddings, batch_first=True).float()\n    batch_size, seq_len, _ = batched_embeddings.shape\n    attention_mask = torch.zeros((batch_size, seq_len), dtype=torch.float)\n    for i, seq in enumerate(embeddings):\n        attention_mask[i, : len(embeddings)] = 1\n\n    return {\n        \"embeddings\": batched_embeddings,\n        \"attention_mask\": attention_mask,\n        \"targets\": torch.Tensor(labels).long(),\n    }\n\ndl = DataLoader(trn_ds, batch_size=3, shuffle=True, collate_fn=collate_fn)\nb = next(iter(dl))\nmodel = TransformerEncoder(4, 512, 128)\nmodel(**b)\n\n# import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments\nfrom torch_snippets.charts import CM\n\nmodel = TransformerEncoder(4, 512, 128).cuda()\n\n\ndef compute_metrics(input):\n    predictions = input.predictions\n    targets = input.label_ids\n    pred = predictions.argmax(1)\n    id2label = trn_ds.id2label if not BINARY_MODE else [\"others\", \"action\"]\n    pred = np.array([id2label[p] for p in pred])\n    targets = np.array([id2label[t] for t in targets])\n    show(CM(pred=pred, truth=targets))\n    return {\"accuracy\": (targets == pred).mean()}\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./transformers_model_trained\",\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    max_steps=2000,\n    logging_steps=200,\n    save_steps=200,\n    save_total_limit=2,\n    label_names=[\"targets\"],\n    include_inputs_for_metrics=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n)\ntraining_args.learning_rate_scheduler = \"cosine_with_restarts\"\ntraining_args.learning_rate_scheduler_kwargs = {\n    \"num_warmup_steps\": 0,  # Adjust this as needed\n    \"num_cycles\": 30,  # Adjust this as needed\n    # \"learning_rate\": 1e-4,\n}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=trn_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\ntrainer.predict(val_ds)\n\n\nmodel = TransformerEncoder(4, 512, 128).cuda()\nload_torch_model_weights_to(model, \"saved-models/a/pytorch_model.bin\")\nmodel.eval()\ntraining_args = TrainingArguments(\n    output_dir=\"./linear_model_trained\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    label_names=[\"targets\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n)\n\npredictions = trainer.evaluate(val_ds)\npredictions = trainer.evaluate(ds)\n\n\nimport nbdev\n\nnbdev.nbdev_export()"
  },
  {
    "objectID": "model/infer.html",
    "href": "model/infer.html",
    "title": "Infer using a Transformer",
    "section": "",
    "text": "from nbdev.showdoc import *\nimport sys\n\n__root = \"../../\"\nsys.path.append(__root)\n\nUsage\ninference = Inference(\"/mnt/347832F37832B388/projects/ssbd/cogniable-assignment/artifacts/a/pytorch_model.bin\")\nvideo_path = \"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-raw-videos/v_ArmFlapping_09.mp4\"\ncontent = inference.predict_on_video_path(video_path)\n\nimport nbdev\n\nnbdev.nbdev_export()"
  }
]