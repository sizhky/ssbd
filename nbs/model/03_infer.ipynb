{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer using a Transformer\n",
    "\n",
    "> Module for training on a dataset of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext nb_black\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "import sys\n",
    "\n",
    "__root = \"../../\"\n",
    "sys.path.append(__root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from clip_video_classifier.preprocess.video_to_frames import (\n",
    "    video_to_frames,\n",
    "    get_video_duration,\n",
    ")\n",
    "from clip_video_classifier.models.transformer_encoder import (\n",
    "    TransformerEncoder,\n",
    "    collate_fn,\n",
    ")\n",
    "from clip_video_classifier.models.frame_embeddings import Frame2Embeddings\n",
    "from clip_video_classifier.data.dataset import ClipEmbeddingsDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch_snippets import *\n",
    "from functools import lru_cache\n",
    "from torch_snippets.trainer import to\n",
    "from clip_video_classifier.preprocess.create_annotations import wget\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_trained_model(model_path):\n",
    "    makedir(parent(model_path))\n",
    "    if not exists(model_path):\n",
    "        wget(\n",
    "            \"https://huggingface.co/sizhkhy/ssbd-video-classifier/resolve/main/pytorch_model.bin\",\n",
    "            model_path,\n",
    "        )\n",
    "    model = TransformerEncoder(4, 512, 128).cpu()\n",
    "    load_torch_model_weights_to(model, model_path, device=\"cpu\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model_path):\n",
    "        self.model, self.clip = self.load_model_and_clip(model_path)\n",
    "\n",
    "    def load_model_and_clip(self, model_path):\n",
    "        model = download_trained_model(model_path)\n",
    "        clip = Frame2Embeddings(device=\"cpu\")\n",
    "        return model, clip\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_on_video_path(self, video_path, start_sec=None):\n",
    "        if start_sec is None:\n",
    "            duration = get_video_duration(video_path)\n",
    "            output = {}\n",
    "            for start in range(0, int(duration), 5):\n",
    "                output[start] = self.predict_on_video_path(video_path, start_sec=start)\n",
    "            return output\n",
    "        frames = video_to_frames(\n",
    "            video_path, frames_path=None, start_sec=0, clip_duration=5, verbose=True\n",
    "        )\n",
    "        frames = to(frames, self.clip.device)\n",
    "        input = self.clip.frames2clip_image_embeddings(frames)\n",
    "        input = {\"embeddings\": input}\n",
    "        Info(f\"{input=}\")\n",
    "        input = collate_fn([input])\n",
    "        predictions = self.model(**input)\n",
    "        predictions = F.softmax(predictions[\"logits\"], dim=-1)\n",
    "        Info(f\"{predictions=}\")\n",
    "        confidence, predicted_class = predictions.max(1)\n",
    "        predicted_class = ClipEmbeddingsDataset.id2label[predicted_class.item()]\n",
    "        confidence = confidence.item()\n",
    "        Info(f\"{predicted_class=} @ {confidence=}\")\n",
    "        return {\n",
    "            \"predicted_class\": predicted_class,\n",
    "            \"confidence\": f\"{confidence:.2f}\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = Inference(\n",
    "    \"/mnt/347832F37832B388/projects/ssbd/cogniable-assignment/artifacts/a/pytorch_model.bin2\"\n",
    ")\n",
    "video_path = (\n",
    "    \"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-raw-videos/v_ArmFlapping_09.mp4\"\n",
    ")\n",
    "content = inference.predict_on_video_path(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Usage\n",
    "\n",
    "```python\n",
    "inference = Inference(\"/mnt/347832F37832B388/projects/ssbd/cogniable-assignment/artifacts/a/pytorch_model.bin\")\n",
    "video_path = \"/mnt/347832F37832B388/ml-datasets/ssbd/ssbd-raw-videos/v_ArmFlapping_09.mp4\"\n",
    "content = inference.predict_on_video_path(video_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcvp-book",
   "language": "python",
   "name": "mcvp-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
